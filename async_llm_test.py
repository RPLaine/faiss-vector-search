#!/usr/bin/env python3
"""
Async LLM Test Script

This script makes an asynchronous request to the LLM API using the configuration
from config.json with a test prompt. Uses async/await for non-blocking requests.
"""

import json
import aiohttp
import aiofiles
import asyncio
import time


async def load_config(config_path="config.json"):
    """Load configuration from JSON file asynchronously."""
    async with aiofiles.open(config_path, 'r', encoding='utf-8') as f:
        content = await f.read()
        return json.loads(content)


async def make_llm_request_async(prompt, config):
    """Make an async request to the external LLM API."""
    llm_config = config["external_llm"]
    
    # Get API details from config
    api_url = llm_config["url"]
    payload_type = llm_config.get("payload_type", "prompt")
    
    # Prepare payload based on type
    if payload_type == "message":
        # OpenAI-style message format
        payload = {
            "model": llm_config["model"],
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            "max_tokens": llm_config["max_tokens"],
            "temperature": llm_config.get("temperature", 0.7)
        }
    else:
        # Ollama-style prompt format (default)
        payload = {
            "model": llm_config["model"],
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": llm_config["max_tokens"],
                "temperature": llm_config.get("temperature", 0.7)
            }
        }
    
    # Prepare headers
    headers = llm_config.get("headers", {})
    if not headers:
        headers = {"Content-Type": "application/json"}
    
    print(f"üåê Making async request to: {api_url}")
    print(f"üìù Model: {llm_config['model']}")
    print(f"üîß Payload type: {payload_type}")
    print(f"üìä Max tokens: {llm_config['max_tokens']}")
    print("=" * 60)
    
    # Print exact lengths for debugging
    if payload_type == "message":
        message_content = payload["messages"][0]["content"]
        print(f"üìè EXACT LENGTHS:")
        print(f"   üìù Prompt length: {len(prompt)} characters")
        print(f"   üí¨ Message content length: {len(message_content)} characters")
        print(f"   üì¶ Full payload length: {len(json.dumps(payload))} characters")
    else:
        print(f"üìè EXACT LENGTHS:")
        print(f"   üìù Prompt length: {len(prompt)} characters") 
        print(f"   üì¶ Full payload length: {len(json.dumps(payload))} characters")
    print("=" * 60)
    
    # Pretty print the request payload
    print("üì§ REQUEST PAYLOAD:")
    print(json.dumps(payload, indent=2, ensure_ascii=False))
    print("=" * 60)
    
    try:
        start_time = time.time()
        
        # Use aiohttp for async HTTP requests
        timeout = aiohttp.ClientTimeout(total=llm_config["timeout"])
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.post(
                api_url,
                json=payload,
                headers=headers
            ) as response:
                processing_time = time.time() - start_time
                
                # Check if request was successful
                response.raise_for_status()
                result = await response.json()
                
                print(f"‚úÖ Async request successful! ({processing_time:.2f}s)")
                print("=" * 60)
                
                # Pretty print the raw API response
                print("üì• RAW API RESPONSE:")
                print(json.dumps(result, indent=2, ensure_ascii=False))
                print("=" * 60)
                
                # Extract response text based on payload type
                if payload_type == "message":
                    # OpenAI-style response format
                    if "choices" in result and len(result["choices"]) > 0:
                        response_text = result["choices"][0]["message"]["content"].strip()
                    elif "content" in result:
                        response_text = result["content"].strip()
                    else:
                        response_text = f"‚ùå Unexpected message API response format: {result}"
                else:
                    # Ollama-style response format (default)
                    if "response" in result:
                        response_text = result["response"].strip()
                    else:
                        response_text = f"‚ùå Unexpected prompt API response format: {result}"
                
                return response_text, processing_time
        
    except (aiohttp.ClientError, asyncio.TimeoutError) as e:
        processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
        error_msg = f"‚ùå Error communicating with LLM API: {e}"
        print(error_msg)
        return error_msg, processing_time


async def run_test(test_name, prompt, config):
    """Run a single test with the given prompt."""
    print(f"\nüß™ TEST: {test_name}")
    print("=" * 80)
    print(f"üìù Prompt length: {len(prompt)} characters")
    print("üîÑ Sending async request...")
    print("=" * 60)
    
    # Make the async request
    response_text, processing_time = await make_llm_request_async(prompt, config)
    
    # Display extracted response
    print(f"üìã EXTRACTED RESPONSE ({test_name}):")
    print("=" * 60)
    print(response_text)
    print("=" * 60)
    print(f"‚è±Ô∏è  Processing time: {processing_time:.2f} seconds")
    print(f"üìä Response length: {len(response_text)} characters")
    
    return response_text, processing_time


async def main():
    """Main async function to test the LLM API with two scenarios."""
    print("üöÄ Async LLM Dual Test Script")
    print("=" * 80)
    print("üéØ Running two tests: FAISS Enhanced (with context) vs Direct LLM (no context)")
    print("=" * 80)
    
    # Load configuration asynchronously
    try:
        config = await load_config()
        print("‚úÖ Configuration loaded successfully")
    except Exception as e:
        print(f"‚ùå Failed to load config: {e}")
        return
    
    # Base question without context
    base_question = """# Kysymys
Mit√§ pidet√§√§n t√§rkeimp√§n√§ asiana?

# Ohjeet
Vastaa kysymykseen.
K√§yt√§ kontekstia, jos se on saatavilla."""
    
    # Test 1: FAISS Enhanced (with context)
    faiss_prompt = """QUERY METADATA
================
Timestamp: 2025-09-30 15:27:40
Template: basic_rag
Original Query: Mit√§ pidet√§√§n t√§rkeimp√§n√§ asiana?

FORMATTED PROMPT (WITH FAISS CONTEXT)
=====================================
<konteksti>

Source File: L1.txt
Chunk: 3 of 19
Lines: 28-52
Characters: 1896
Type: Conversational
--------------------------------------------------

K: [naurahtaa] No mutta katotaan mihin p√§√§st√§√§n. Mul on t√§ss√§ muutamia kysymyksi√§. Niin otetaan ihan alkuun vapaa sana. Miten s√§ k√§sit√§t jutun k√§rjen? Mit√§ se tarkoittaa?

V: Semmosessa uutisrakennetta noudattavassa tekstiss√§ jutun k√§rkeen laitetaan, eli siis siihen siis jutun alkuun, t√§lleen yleist√§en sanottuna, laitetaan ne t√§rkeimm√§t uudet asiat, joita se uutinen tai juttu tuo julki.

K: Elikk√§ hahmotat sen t√§llasena rakenteellisena asiana?

V: Joo.

K: Ennemminkin ku sis√§ll√∂llisen√§ vai? Kumpi (--) [0:04:23 pp]

V: Rakenteellisena m√§ sen enemm√§n hahmotan. Tai mun mielest√§ k√§rki ja n√§k√∂kulma on ehk√§ eri asia.

K: Niinp√§. Joo. K√§rki, n√§k√∂kulma, idea. T√§ss√§ ehk√§ v√§h√§n liipataan n√§it√§ samoja. Miten... Tai ne limittyv√§t tietysti. Miten s√§ hahmottaisit sitten, ett√§ miten t√§√§ k√§rki eroaa siit√§ n√§k√∂kulmasta?

V: Mun mielest√§ uutisk√§rki on ehk√§ enemm√§n semmonen tekninen ratkaisu. Et mit√§ sielt√§ jutusta poimitaan siihen, nostetaan t√§rkeimm√§ks. Ja n√§k√∂kulma taas on semmonen, ett√§ mist√§, mill√§ tavalla sit√§ aihetta k√§sitell√§√§n. Semmosessa, uutisten joukossa, kun on monta juttua, jotka k√§sittelee samaa aihetta, niin sitten tietysti sit√§ pit√§√§ k√§sitell√§ erilaisista n√§k√∂kulmista.

K: Ent√§, miten se eroaa niink√∂ jutun ideasta sitten?

V: Joo, m√§ en tii√§, ehk√§ se jutun idea on sitten laajempi asia.

K: Miten sanottaisit, millainen on hyv√§ jutun k√§rki?

V: No... Ehk√§ uutisissa se on informatiivinen ja semmonen, ett√§ lukija saa jo, p√§√§see jo heti jutun alusta kartalle siit√§, ett√§ mit√§ se k√§sittelee, se uutinen, ja miksi se on uutinen. Ne t√§rkeimm√§t asiat on siin√§ k√§rjess√§. Mut sitten pidemm√§ss√§ jutussa, niin se k√§rki tai alku voi olla my√∂s semmonen, joka jotenkin koukuttaa sen lukijan. Houkuttelee lukemaan sit√§ pidemm√§lle. Se voi olla my√∂s hauska tai outo.

K: Elikk√§ jos on t√§llanen feature-p√§tk√§, niin sitten miten n√§it√§ sitten sanottaisit, ett√§ mink√§laisia k√§rki√§ niiss√§ voi olla?

</konteksti>

""" + base_question
    
    # Run both tests
    try:
        faiss_response, faiss_time = await run_test("FAISS Enhanced (with context)", faiss_prompt, config)
        
        # Comparison summary
        print("\nÔøΩ COMPARISON SUMMARY")
        print("=" * 80)
        print(f"üîç FAISS Enhanced:")
        print(f"   ‚è±Ô∏è  Time: {faiss_time:.2f}s")
        print(f"   üìè Length: {len(faiss_response)} chars")
        print(f"   üìù Prompt: {len(faiss_prompt)} chars (with context)")
        print("\n‚úÖ Async test completed!")
        
    except Exception as e:
        print(f"‚ùå Test execution failed: {e}")


if __name__ == "__main__":
    # Run the async main function
    asyncio.run(main())